from __future__ import print_function, division, unicode_literals
import numpy as np
import tensorflow as tf


# USEFUL LAYERS
fc = tf.contrib.layers.fully_connected
conv = tf.contrib.layers.conv2d
# convsep = tf.contrib.layers.separable_conv2d
deconv = tf.contrib.layers.conv2d_transpose
relu = tf.nn.relu
maxpool = tf.contrib.layers.max_pool2d
dropout_layer = tf.layers.dropout
batchnorm = tf.contrib.layers.batch_norm
winit = tf.contrib.layers.xavier_initializer()
repeat = tf.contrib.layers.repeat
arg_scope = tf.contrib.framework.arg_scope
l2_regularizer = tf.contrib.layers.l2_regularizer


def get_conv_arg_scope(is_training, bn=True, reg=None, use_deconv=False, use_relu=True, bn_decay=0.9):
    with arg_scope(
        [deconv if use_deconv else conv],
        padding = "SAME",
        stride = 1,
        activation_fn = relu if use_relu else None,
        normalizer_fn = batchnorm if bn else None,
        normalizer_params = {"is_training": is_training, "decay": bn_decay},
        weights_regularizer = reg,
        variables_collections = None,
        ) as scope:
        return scope


def factorized_res_moduleOLD(x, is_training, dropout=0.3, dilation=1, name="fres"):
    with arg_scope(get_conv_arg_scope(is_training=is_training, bn=True)):
        with tf.variable_scope(name):
            n_filters = x.shape.as_list()[-1]
            y = conv(x, num_outputs=n_filters, kernel_size=[3,1], normalizer_fn=None, scope="conv_a_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], scope="conv_a_1x3")
            y = conv(y, num_outputs=n_filters, kernel_size=[3,1], rate=dilation, normalizer_fn=None, scope="conv_b_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], rate=dilation, scope="conv_b_1x3")
            y = dropout_layer(y, rate=dropout)
            y = tf.add(x,y, name="add")
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    return y


def factorized_res_module(x, is_training, dropout=0.3, dilation=[1,1], l2=None, name="fres"):
    reg = None if l2 is None else l2_regularizer(l2)
    with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=True)):
        with tf.variable_scope(name):
            n_filters = x.shape.as_list()[-1]

            # Todo: check if dilation is performed correctly. Appears like it is set on the wrong axe
            # Todo: layer order is not the same w.r.t. the official pytorch implementation, and batch norm is not used
            y = conv(x, num_outputs=n_filters, kernel_size=[3,1], rate=dilation[0], normalizer_fn=None, scope="conv_a_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], rate=dilation[0], scope="conv_a_1x3")
            y = conv(y, num_outputs=n_filters, kernel_size=[3,1], rate=dilation[1], normalizer_fn=None, scope="conv_b_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], rate=dilation[1], scope="conv_b_1x3")
            y = dropout_layer(y, rate=dropout)
            y = tf.add(x, y, name="add")
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    print("DEBUG: L2 in factorized res module {}".format(l2))
    return y


def non_bottleneck_1d(x, is_training, dropout=0.3, dilation=[1,1], l2=None, curr_layer_num=3):
    reg = None if l2 is None else l2_regularizer(l2)
    name = f'fres{curr_layer_num}'
    with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=True)):
        with tf.variable_scope(name):
            n_filters = x.shape.as_list()[-1]

            # Todo: check if dilation is performed correctly. Appears like it is set on the wrong axe
            # Todo: layer order is not the same w.r.t. the official pytorch implementation, and batch norm is not used
            y = conv(x, num_outputs=n_filters, kernel_size=[3,1], padding='SAME',
                     scope="conv_a_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], padding='SAME',
                     scope="conv_a_1x3")
            y = batchnorm(y, variance_epsilon=1e-03)

            y = conv(y, num_outputs=n_filters, kernel_size=[3,1], rate=dilation[1], normalizer_fn=None, scope="conv_b_3x1")
            y = conv(y, num_outputs=n_filters, kernel_size=[1,3], rate=dilation[1], scope="conv_b_1x3")

            y = batchnorm(y, variance_epsilon=1e-03)
            y = dropout_layer(y, rate=dropout)
            y = tf.add(x, y, name="add")

    curr_layer_num += 1
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    print("DEBUG: L2 in factorized res module {}".format(l2))
    return y, curr_layer_num


def downsample(x, n_filters, is_training, bn=False, use_relu=False, l2=None, name="down"):
    with tf.variable_scope(name):
        reg = None if l2 is None else l2_regularizer(l2)
        with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=bn, use_relu=use_relu)):
            n_filters_in = x.shape.as_list()[-1]
            n_filters_conv = n_filters - n_filters_in  # this because we are going to concatenate msx pooling layers
            # Todo: what about padding=1?
            branch_a = conv(x, num_outputs=n_filters_conv, kernel_size=3, stride=2, scope="conv")
            branch_b = maxpool(x, kernel_size=2, stride=2, padding='VALID', scope="maxpool")
            y = tf.concat([branch_a, branch_b], axis=-1, name="concat")
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    return y


def upsample(x, n_filters, is_training=False, use_relu=False, bn=False, l2=None, name="up"):
    reg = None if l2 is None else l2_regularizer(l2)
    with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=bn, use_deconv=True, use_relu=use_relu)):
        # Todo: why kernel size is 4? In the orig. paper is 3
        y = deconv(x, num_outputs=n_filters, kernel_size=4, stride=2, scope=name)
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    return y


def downsample_new(x, n_filters, is_training, bn=False, use_relu=False, l2=None, curr_layer_num=1):
    name = f'down{curr_layer_num}'
    with tf.variable_scope(name):
        reg = None if l2 is None else l2_regularizer(l2)
        with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=bn, use_relu=use_relu)):
            n_filters_in = x.shape.as_list()[-1]
            n_filters_conv = n_filters - n_filters_in  # this because we are going to concatenate msx pooling layers
            # Todo: what about padding=1?
            branch_a = conv(x, num_outputs=n_filters_conv, kernel_size=3, stride=2, scope="conv")
            branch_b = maxpool(x, kernel_size=2, stride=2, padding='VALID', scope="maxpool")
            y = tf.concat([branch_a, branch_b], axis=-1, name="concat")
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    return y


def upsample_new(x, n_filters, is_training=False, use_relu=False, bn=False, l2=None, curr_layer_num=17):
    reg = None if l2 is None else l2_regularizer(l2)
    name = f'up{curr_layer_num}'
    with arg_scope(get_conv_arg_scope(reg=reg, is_training=is_training, bn=bn, use_deconv=True, use_relu=use_relu)):
        # Todo: why kernel size is 4? In the orig. paper is 3
        y = deconv(x, num_outputs=n_filters, kernel_size=4, stride=2, scope=name)
    print("DEBUG: {} {}".format(name, y.shape.as_list()))
    return y


def romeranetB_logits(X, Y, n_classes, alpha=0.001, dropout=0.3, l2=None, is_training=False):
    """
    """
    # TODO: Use TF repeat for some of these repeating layers
    # TODO: register factorized_res_module and upsample and downsample with arg_scope
    #       and pass dropout, is_training, etc to it.
    # TODO: Add weight decay.
    with tf.name_scope("preprocess") as scope:
        x = tf.div(X, 255., name="rescaled_inputs")

    x = downsample(x, n_filters=16, is_training=is_training, name="d1")

    x = downsample(x, n_filters=64, is_training=is_training, name="d2")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres3")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres4")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres5")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres6")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres7")

    # TODO: Use dilated convolutions
    x = downsample(x, n_filters=128, is_training=is_training, name="d8")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=2, name="fres9")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=4, name="fres10")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=8, name="fres11")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=16, name="fres12")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=2, name="fres13")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=4, name="fres14")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=8, name="fres15")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=16, name="fres16")

    x = upsample(x, n_filters=64, is_training=is_training, name="up17")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres18")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres19")

    x = upsample(x, n_filters=16, is_training=is_training, name="up20")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres21")
    x = factorized_res_moduleOLD(x, is_training=is_training, dilation=1, name="fres22")

    x = upsample(x, n_filters=n_classes, is_training=is_training, name="up23")
    return x


def erfnetA(X, Y, n_classes, alpha=0.001, dropout=0.3, l2=None, is_training=False):
    """
    """
    # TODO: Use TF repeat for some of these repeating layers
    # TODO: register factorized_res_module and upsample and downsample with arg_scope
    #       and pass dropout, is_training, etc to it.
    # TODO: Add weight decay.
    with tf.name_scope("preprocess") as scope:
        x = tf.div(X, 255., name="rescaled_inputs")

    x = downsample(x, n_filters=16, is_training=is_training, name="d1")

    x = downsample(x, n_filters=64, is_training=is_training, name="d2")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres3")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres4")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres5")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres6")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres7")

    # TODO: Use dilated convolutions
    x = downsample(x, n_filters=128, is_training=is_training, name="d8")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 2], name="fres9")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 4], name="fres10")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 8], name="fres11")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 16], name="fres12")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 2], name="fres13")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 4], name="fres14")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 8], name="fres15")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 16], name="fres16")

    x = upsample(x, n_filters=64, is_training=is_training, name="up17")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres18")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres19")

    x = upsample(x, n_filters=16, is_training=is_training, name="up20")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres21")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], name="fres22")

    x = upsample(x, n_filters=n_classes, is_training=is_training, name="up23")
    return x


def erfnetB(X, Y, n_classes, alpha=0.001, dropout=0.3, l2=None, is_training=False):
    """
    Uses L2 regularization.
    """
    print("DEBUG: L2 passed on to ERFNETB{}".format(l2))
    # TODO: Use TF repeat for some of these repeating layers
    # TODO: register factorized_res_module and upsample and downsample with arg_scope
    #       and pass dropout, is_training, etc to it.
    # TODO: Add weight decay.
    with tf.name_scope("preprocess") as scope:
        x = tf.div(X, 255., name="rescaled_inputs")

    # ENCODER
    x = downsample(x, n_filters=16, is_training=is_training, l2=l2, name="d1")

    x = downsample(x, n_filters=64, is_training=is_training, l2=l2, name="d2")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres3")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres4")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres5")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres6")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres7")

    # TODO: Use dilated convolutions
    x = downsample(x, n_filters=128, is_training=is_training, l2=l2, name="d8")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 2], l2=l2, name="fres9")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 4], l2=l2, name="fres10")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 8], l2=l2, name="fres11")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 16], l2=l2, name="fres12")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 2], l2=l2, name="fres13")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 4], l2=l2, name="fres14")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 8], l2=l2, name="fres15")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 16], l2=l2, name="fres16")

    # DECODER
    x = upsample(x, n_filters=64, is_training=is_training, l2=l2, name="up17")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres18")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres19")

    x = upsample(x, n_filters=16, is_training=is_training, l2=l2, name="up20")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres21")
    x = factorized_res_module(x, is_training=is_training, dilation=[1, 1], l2=l2, name="fres22")

    x = upsample(x, n_filters=n_classes, is_training=is_training, l2=l2, name="up23")
    return x


def encoder(x, is_training, l2, num_bt_1=5, num_bt_2=2, curr_layer_num=3):
    x, curr_layer_num = downsample_new(x, n_filters=16, is_training=is_training,
                                       l2=l2, curr_layer_num=curr_layer_num)
    x, curr_layer_num = downsample_new(x, n_filters=64, is_training=is_training,
                                       l2=l2, curr_layer_num=curr_layer_num)

    for n_layer_1 in range(num_bt_1):
        x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training, dilation=[1, 1],
                                              l2=l2, curr_layer_num=curr_layer_num)

    x = downsample(x, n_filters=128, is_training=is_training, l2=l2, name="d8")
    for n_layer_2 in range(num_bt_2):
        x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training,
                                              dilation=[1, 2], l2=l2, curr_layer_num=curr_layer_num)
        x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training, dilation=[1, 4],
                                              l2=l2, curr_layer_num=curr_layer_num)
        x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training, dilation=[1, 8],
                                              l2=l2, curr_layer_num=curr_layer_num)
        x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training, dilation=[1, 16],
                                              l2=l2, curr_layer_num=curr_layer_num)

    return x, curr_layer_num


def decoder(x, is_training, l2, num_dec_routine, num_bt, curr_layer_num, n_filters=64): # num_bt=2
    for iter in range(num_dec_routine):
        n_filters /= iter # first 64, second 16
        x = upsample_new(x, n_filters=n_filters, is_training=is_training, l2=l2, curr_layer_num=curr_layer_num)

        for n_layer in range(num_bt):
            x, curr_layer_num = non_bottleneck_1d(x, is_training=is_training, dilation=[1, 1],
                                                  l2=l2, curr_layer_num=curr_layer_num)

    return x, curr_layer_num


def erfnetNEW(X, Y, n_classes, alpha=0.001, dropout=0.3, l2=None, is_training=False):
    """
    Uses L2 regularization.
    """
    print("DEBUG: L2 passed on to ERFNET_NEW{}".format(l2))
    # TODO: Use TF repeat for some of these repeating layers
    # TODO: register factorized_res_module and upsample and downsample with arg_scope
    #       and pass dropout, is_training, etc to it.
    # TODO: Add weight decay.
    with tf.name_scope("preprocess") as scope:
        x = tf.div(X, 255., name="rescaled_inputs")

    x, curr_layer_num = encoder(x, is_training, l2, num_bt_1=5, num_bt_2=2, curr_layer_num=3)
    x, curr_layer_num = decoder(x, is_training, l2, num_dec_routine=2, num_bt=2,
                                curr_layer_num=curr_layer_num, n_filters=64)
    return x


